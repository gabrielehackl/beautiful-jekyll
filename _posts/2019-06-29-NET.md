---
layout: post
title: Edges and Nodes
subtitle: generating files for Gephi




---
What had to be done: create a network of places in the “Dispatch”, if places are mentioned in the same article, they must be connected;
What happended: I wrote two scripts, the first generates the edges between the article and the place (tgn), the second one generates the data for the nodes/places (names, coordinates);

**Script I:**

Script I loops through all the dispatch-files (xml-format) and creates an output for all the **articles** of **one year**, the output being: a tsv-file with the colums "source", "target" and "weight"; to create the source I am putting the date and an article-counter in an ID; for the target I am using the tgn-number; in order to not have the weight but not any duplicates in the final list I am counting the frequency of occurences, only writing the first instance in a dictionary and counting up all further instances; when writing those informations in a tsv-file, I only write the dict-entries with a weight **as high or higher than ten**;

~~~
import re, os, csv

source = "C:/Users/hackl/Documents/Seminare/DH_Methodenkurs/MAPPING2/XMLfiles/" #path to the xml-files with the coordinates
oldPath = "C:/Users/hackl/Documents/Seminare/DH_Methodenkurs/TSV2/Homework/" #path to the original dispatch xml-files
newPath = "C:/Users/hackl/Documents/Seminare/DH_Methodenkurs/TOPIC/"#new path to save the new tsv

lof = os.listdir(oldPath) #creating a list of files from the dispatch xmls

def generate(filter): #function where filter will be our input (the year), cause the dispatch is to big to be handled at once
    dataList = {}
    for f in lof: #looping through our XML files
        if f.startswith("dltext"): # fileName test        
            with open(oldPath + f, "r", encoding="utf8") as f1:
                text = f1.read() #getting the content of the file
                text = text.replace("&amp;", "&") #cleaning the content of the file
                # try to find the date:
                date = re.search(r'<date value="([\d-]+)"', text).group(1)

                if date.startswith(filter):# making sure the output only includes data for the input year
                    c = 0   
                    allarticles = re.findall(r'(?s)<div3 type="article"(.*?)</div3>', text)
                    for article in allarticles:
                        c += 1                                        
                        # creating an ID:
                        ID = date+"_article_"+str(c)
                        for tg in re.findall(r"(tgn,\d+)", article):#finding the tgn info
                            tgn = tg.split(",")[1] #reducing the tgn info to the tgn-number
                            key = ID + '##' + tgn#creating a key from ID and tgn to count the frequency of the occurence
                            #writing the dict, when duplicates are there count up the frequency, otherwise write 1:
                            if key in dataList:
                                dataList[key]['weight']+= 1
                            else:
                                dataList[key] = {
                                    'source': ID,
                                    'target': tgn,
                                    'weight': 1
                                }            

    #saving the data in csv format:
    tsv_columns =  ['source', 'target','weight']
    with open("dispatch_toponymsID_%s.tsv" % filter, "w", newline='', encoding="utf8") as f9:
            writer = csv.DictWriter(f9, delimiter ='\t',fieldnames=tsv_columns)
            writer.writeheader()
            for data in dataList:
                if dataList[data]['weight'] >= 10:
                    writer.writerow(dataList[data])

#using our function for different years, choosing 1861:
generate("1861")
~~~

The output for the edges looks like this:

![image output_edges](/img/output_edges.png)

**Script II:**

For Script II I reused an older script (Mapping2) that matches tgn-numbers from the articles with those of a Gazzetter, so that the coordinates can be extracted; the major change of the script is, that this time I write the places without coordinates into my output tsv-file as well, 'cause even if there are no coords, I want the place name for the labeling of knots in Gephi later on;

~~~
import re, os, csv

#loading edges-data into a dictionary (with a function):
def loadTGN(tgnTSV):
    with open(tgnTSV, "r", encoding="utf8") as f1:
        data = f1.read().split("\n")
        dic = {}#creating the dict we will use

        for d in data:#looping through every line
            d = d.split("\t")#seperating the data at TABs
            dic[d[0]] = d#filling the dict with the keys and values
    return(dic)

#matching our data-files/tgn with the edges-data/tgn (with a function):
def match(freqFile, dicToMatch):
    with open(freqFile, "r", encoding="utf8") as f1:
        data = f1.read().split("\n")
        #creating all the lists we need:    
        dataNew = []
        dataNewNA = []

        for d in data[1:]:#looping through our edges-file
            try:
                tgnID = d.split("\t")[1]#getting the tgn-number so we can compare it to the one in the dict
                if tgnID in dicToMatch:#matching places to the dict by tgn-number
                    val = "\t".join(dicToMatch[tgnID])#variable for the entry in the dict
                    if "\tNA\t" in val:#creating lists with the toponymes with and without any coordinates
                        dataNew.append(val)#appending all the places to the list (also those without coord) in case gephi does not like to have missing knots
                        dataNewNA.append(val)
                    else:
                        dataNew.append(val)
                print("%s not in Gaz!" % (tgnID))#states which tgnID could not be found in Gazzetter
            except: print("ungültiger Inhalt: "+d)    

    header = "ID\tLabel\tlat\tlon\n"
    #creating a file with all the toponymes with their coordinates:
    with open("nodes_"+freqFile, "w", encoding="utf8") as f9a:
        f9a.write(header + "\n".join(dataNew))
    #creating a file with all the toponymes without coordinates:
    with open("nodes_onlyNA_"+freqFile, "w", encoding="utf8") as f9b:
        f9b.write(header + "\n".join(dataNewNA))

dictionary = loadTGN("tgn_data_light.tsv")#variable for the Getty Gazetteer we cleaned up
#running the two functions using our toponyms and the "tgn_data_light.tsv"
match("dispatch_toponymsID_1861.tsv", dictionary)
~~~

The output for the nodes (as long as they are places) looks like this:

![image output_nodes](/img/output_nodes.png)
